{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import jieba\n",
    "# 调用 jieba分词module后，添加单词本（人名等）:\n",
    "jieba.load_userdict(\"data/names_separate.txt\")\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. 数据处理和准备阶段\n",
    "\n",
    "### 1） 输入原始文档\n",
    "\n",
    "我的实验里面读入了《冰与火之歌》中文版的前5本，版权原因，我们不提供txt文件（各位可以自己搜索txt文件，逃。。。)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "88765it [00:00, 1206561.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总共读入88765行文字\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "filename = 'data/ice_and_fire_utf8.txt'\n",
    "text_lines = []\n",
    "\n",
    "with open(filename, 'r') as f:\n",
    "    for line in tqdm(f):\n",
    "        text_lines.append(line)\n",
    "print('总共读入%d行文字'% (len(text_lines)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2）调用jieba分词工具将句子切分成词语的序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88765/88765 [00:14<00:00, 6199.62it/s]\n"
     ]
    }
   ],
   "source": [
    "data_words, data_lines = [], []\n",
    "# data_words: 训练我们的cbow和skip-gram模型\n",
    "# data_lines: 调用gensim.word2vec训练word2vec模型\n",
    "\n",
    "## 分词:\n",
    "for line in tqdm(text_lines):\n",
    "    one_line = [' '.join(jieba.cut(line, cut_all=False))][0].split(' ')\n",
    "    data_words.extend(one_line)\n",
    "    data_lines.append(one_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3） 去掉标点和数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# 标点符号 (punctuation)\n",
    "punct = set(u''':!),.:;?]}¢'\"、。〉》」』】〕〗〞︰︱︳﹐､﹒﹔﹕﹖﹗﹚﹜﹞！），．＊：；Ｏ？｜｝︴︶︸︺︼︾﹀﹂﹄﹏､～￠々‖•·ˇˉ―--′’”([{£¥'\"‵〈《「『【〔〖（［｛￡￥〝︵︷︹︻︽︿﹁﹃﹙﹛﹝（｛“‘-—_…０１２３４５６７８９''')\n",
    "isNumber = re.compile(r'\\d+.*')\n",
    "\n",
    "filter_words = [w for w in data_words if (w not in punct)\n",
    "                and (not isNumber.search(w.lower()))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4）建立词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41973\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocabulary_size = 30000\n",
    "def build_vocab(words):\n",
    "    \"\"\"对文字数据中最常见的单词建立词典\n",
    "    \n",
    "    Arguments:\n",
    "        words: 一个list的单词,来自分词处理过的文本数据库.\n",
    "    \n",
    "    Returns:\n",
    "        data: 输入words数字编码后的版本\n",
    "        count: dict, 单词 --> words中出现的次数\n",
    "        dictionary: dict, 单词 --> 数字ID的字典\n",
    "        reverse_dictionary: dict, 数字ID-->单词的字典\n",
    "    \"\"\"\n",
    "    # 1. 统计每个单词的出现次数\n",
    "    words_counter = Counter(filter_words)\n",
    "    # 2. 选取常用词\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(words_counter.most_common(vocabulary_size - 1))\n",
    "    # 3. 词语编号\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    # 4. 引入特殊词语UNK\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count = unk_count + 1\n",
    "        data.append(index)\n",
    "    \n",
    "    print(unk_count)\n",
    "    count[0][1] = unk_count\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "\n",
    "# 生成词典\n",
    "data, count, dictionary, reverse_dictionary = build_vocab(filter_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5） 打印一些信息，简单地人工检查有没有明显错误"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共有 167 万个词语\n",
      "前十个单词是 冰 与 火之歌 全集 卷 实体书 精校 版 \n",
      " 作者\n"
     ]
    }
   ],
   "source": [
    "print('共有 %d 万个词语' % (len(filter_words)//10000))\n",
    "print('前十个单词是 %s' % ' '.join(filter_words[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最常见的5个词，包括换行符:\n",
      "0: 的 91563\n",
      "1: \n",
      " 88764\n",
      "2: 他 39071\n",
      "3: 我 28094\n",
      "4: 了 24603\n",
      "\n",
      "\n",
      "出现频率居中的一些词语:\n",
      "1111: 才能 181\n",
      "1211: 金 166\n",
      "1311: 有时 154\n",
      "1411: 记住 145\n",
      "1511: 向前 135\n",
      "1611: 那边 127\n",
      "1711: 贝沃斯 120\n",
      "1811: 发 113\n",
      "1911: 工作 107\n",
      "\n",
      "\n",
      "出现频率万名开外的一些词语\n",
      "10000: 自相残杀 15\n",
      "20000: 系到 5\n",
      "30000: 下望 2\n",
      "40000: 石头堆 1\n",
      "50000: 森和卡 1\n",
      "60000: 中见 1\n"
     ]
    }
   ],
   "source": [
    "# 根据出现频率排列单词\n",
    "from collections import Counter\n",
    "words_counter = Counter(filter_words)\n",
    "word_sorted = []\n",
    "for (k,v) in words_counter.most_common(len(words_counter)):\n",
    "    word_sorted.append((k,v))\n",
    "\n",
    "# 看有哪些词，词频多少\n",
    "# 是否常见词的词频高，生僻词的词频低\n",
    "print('最常见的5个词，包括换行符:')\n",
    "for i in range(5):\n",
    "    print('%d: %s %d' % (i, word_sorted[i][0], word_sorted[i][1]))\n",
    "\n",
    "print('\\n')\n",
    "print('出现频率居中的一些词语:')\n",
    "for i in range(1111, 2000, 100):\n",
    "    print('%d: %s %d' % (i, word_sorted[i][0], word_sorted[i][1]))\n",
    "\n",
    "print('\\n')\n",
    "print('出现频率万名开外的一些词语')\n",
    "for i in range(10000, len(word_sorted), 10000):\n",
    "    print('%d: %s %d' % (i, word_sorted[i][0], word_sorted[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "史塔克 出现 1548 次\n",
      "兰尼斯特 出现 1439 次\n",
      "龙之母 出现 58 次\n"
     ]
    }
   ],
   "source": [
    "# 验证输入jieba的一些name entity被正确地切分出来\n",
    "\n",
    "demo_names = ['史塔克',\n",
    "              '兰尼斯特',\n",
    "              '龙之母']\n",
    "for name in demo_names:\n",
    "    print('%s 出现 %d 次'%(name, words_counter[name]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2691, 195, 655, 778, 11, 1, 715, 6580, 19, 88, 11, 65, 3925, 3318, 510, 51, 137, 1, 3270, 232, 0, 847, 2691, 5, 533, 7008, 1019, 385, 51, 1, 2301, 7, 7240, 1, 449, 51, 86, 32, 11374, 11375, 362, 5857, 1, 2926, 2822, 711, 859, 11, 1457, 10252, 573, 106, 622, 1653, 5, 1617, 1, 2118, 13644, 2118, 1523, 778, 11, 119, 9374, 2118, 4724, 706, 1, 4224, 8976, 11376, 1160, 1160, 131, 5858, 517, 1, 15870, 7, 1554, 1, 29358, 51, 573, 106, 14671, 5, 231, 2220, 11, 860, 1, 3050, 232, 7, 711, 2691, 113, 1]\n",
      "征服 之 路 尽管 他们 的 军队 数量 不 多 他们 却 有着 西方 世界 中 最后 的 三条 龙 UNK 此 征服 了 整个 大陆 七大 王国 中 的 六个 在 最初 的 战争 中 便 被 降服 唯独 多恩 激烈 的 反抗 以至于 伊耿 同意 他们 保持 独立 坦格利安 家族 同样 放弃 了 原来 的 信仰 改为 信仰 七神 尽管 他们 还是 违背 信仰 按照 瓦雷利亚 的 传统 兄妹 通婚   并 遵守 维斯特洛 的 风俗 在 接下来 的 数十年 中 坦格利安 家族 扑灭 了 所有 反对 他们 统治 的 叛乱 龙 在 伊耿 征服 后 的 \n"
     ]
    }
   ],
   "source": [
    "# 验证单词到数字的编码是正确的\n",
    "\n",
    "demo_num = data[1000:1100]\n",
    "print(demo_num)\n",
    "\n",
    "demo_str = ''\n",
    "for i in range(1000, 1100):\n",
    "    demo_str = demo_str+(reverse_dictionary[data[i]])+' '\n",
    "print(demo_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. 使用word2vec训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.Word2Vec(iter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.build_vocab(data_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14361387"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(data_lines, total_examples = len(data_lines), epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_words = ['史塔克', '提利昂', '琼恩', '长城', '衣物', '力量', '没关系']\n",
    "neighbors = []\n",
    "for test_word in test_words:\n",
    "    neighbors.append(model.most_similar(test_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "史塔克:\n",
      "\t徒利 波顿 艾林 葛雷乔伊 提利尔 兰尼斯特 卢斯 拉萨 戴林恩 佛雷\n",
      "\n",
      "提利昂:\n",
      "\t詹姆 瑟曦 布蕾妮 侏儒 戴佛斯 瓦里斯 小指头 对方 珊莎 波隆\n",
      "\n",
      "琼恩:\n",
      "\t山姆 席恩 耶哥蕊特 熊老 布兰 伊蒙学士 奈德 阿莎 艾莉亚 白灵\n",
      "\n",
      "长城:\n",
      "\t巡逻 城墙 野人 守夜人 先民 拳峰 森林 远远 弟兄们 城堡\n",
      "\n",
      "衣物:\n",
      "\t柴火 长裤 随身携带 钱币 雪橇 煮沸 泥巴 擦洗 带子 全盔\n",
      "\n",
      "力量:\n",
      "\t生命 勇气 死亡 智慧 能力 代价 荣耀 胜利 信仰 人民\n",
      "\n",
      "没关系:\n",
      "\t受不了 一辈子 不行 没用 无所谓 没差 不成问题 不怕 死活 用不着\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(neighbors)):\n",
    "    str = ' '.join([x[0] for x in neighbors[i]])\n",
    "    print('%s:' % test_words[i])\n",
    "    print('\\t%s\\n' % (str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. 使用我们的代码训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III-1. skip-gram模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "np.random.seed(0)\n",
    "\n",
    "def generate_batch_sg(data, batch_size, num_skips, slide_window):\n",
    "    \n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * slide_window\n",
    "    \n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    \n",
    "    # 滑动窗：[ slide_window target slide_window ]，宽度为 span\n",
    "    span = 2 * slide_window + 1\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    \n",
    "    # 扫过文本，将一个长度为 2*slide_window+1 的滑动窗内的词语放入buffer\n",
    "    # buffer里面，居中的是target，“当前单词”\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    \n",
    "    # # 下面的 for 循环：\n",
    "    # 在产生一个有batch_size个样本的minibatch的时候，\n",
    "    # 我们选择 batch_size//num_skips 个 “当前单词” （或者叫“目标单词”）\n",
    "    # 并且从“当前单词”左右的2*slide_window 个词语组成的context里面选择\n",
    "    # num_skips个单词\n",
    "    # “当前单词”（我们叫做x）和num_skips个单词中的每一个（我们叫做y_i）\n",
    "    # 组成一个监督学习样本：\n",
    "    # 给定单词x, 在它的context里面应该大概率地出现单词y_i\n",
    "    for i in range(batch_size // num_skips):\n",
    "        # 在每个长度为2*slide_window + 1 的滑动窗里面，\n",
    "        # 我们选择num_skips个（“当前单词”，“语境单词”）的组合\n",
    "        # 为了凑齐batch_size个组合，我们需要batch_size//num_skips个滑动窗\n",
    "        rand_x = np.random.permutation(span)\n",
    "        j, k = 0, 0\n",
    "        for j in range(num_skips):\n",
    "            while rand_x[k]==slide_window:\n",
    "                k += 1\n",
    "            batch[i * num_skips + j] = buffer[slide_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[rand_x[k]]\n",
    "            k += 1\n",
    "        \n",
    "        # 将滑动窗向右滑动随机步\n",
    "        rand_step = np.random.randint(1,5)\n",
    "        for _ in range(rand_step):\n",
    "            buffer.append(data[data_index])\n",
    "            data_index = (data_index + 1) % len(data)\n",
    "        \n",
    "    return batch, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "with num_skips = 2 and slide_window = 1:\n",
      "UNK --> 成就\n",
      "UNK --> 精心\n",
      "的 --> 成就\n",
      "的 --> 当今世界\n",
      "上 --> 当今世界\n",
      "上 --> 堪称\n",
      "巅峰 --> 的\n",
      "巅峰 --> 奇幻\n",
      "\n",
      "with num_skips = 4 and slide_window = 2:\n",
      "眼中 --> 冰\n",
      "眼中 --> UNK\n",
      "眼中 --> 们\n",
      "眼中 --> 与\n",
      "是 --> UNK\n",
      "是 --> 与\n",
      "是 --> 美国\n",
      "是 --> 火之歌\n"
     ]
    }
   ],
   "source": [
    "# 测试代码：\n",
    "batch_size = 8\n",
    "for num_skips, slide_window in [(2, 1), (4,2)]:\n",
    "    batch, labels = generate_batch_sg(data = data,\n",
    "                                      batch_size=batch_size,\n",
    "                                      num_skips=num_skips,\n",
    "                                      slide_window=slide_window)\n",
    "    print('\\nwith num_skips = %d and slide_window = %d:' % (num_skips, slide_window))\n",
    "    for i in range(batch_size):\n",
    "        print('%s --> %s' % (reverse_dictionary[batch[i]],\n",
    "                             reverse_dictionary[labels[i][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "batch_size = 128\n",
    "num_sampled = 64 # 近似计算cross entropy loss的时候的negative examples参数\n",
    "embedding_size = 128 # Dimension of the embedding vector\n",
    "\n",
    "# skip-gram的两个重要的hyperparameters:\n",
    "# 1. 语境范围：考虑和周围多少个词语的共存关系\n",
    "slide_window = 1\n",
    "# 2.　”使用频率“：（当前单词，临近单词）的组合使用多少次\n",
    "num_skips = 1 # How many times to reuse an input to generate a label\n",
    "\n",
    "# 产生测试数据\n",
    "valid_size = 8\n",
    "valid_examples = list(np.random.permutation(1000)[:valid_size])\n",
    "names = ['史塔克', '提利昂', '琼恩', '长城', '南方', '死亡', '家族', '支持', '愤怒']\n",
    "for name in names:\n",
    "    valid_examples.append(dictionary[name])\n",
    "    valid_size += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_sg = tf.Graph()\n",
    "\n",
    "with graph_sg.as_default():#, tf.device('/cpu:0'):\n",
    "\n",
    "    # Input data.\n",
    "    train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "    # Variables.\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    softmax_weights = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                            stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Model.\n",
    "    # Look up embeddings for inputs.\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "    # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed,\n",
    "                                   labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n",
    "\n",
    "    # Optimizer.\n",
    "    # Note: The optimizer will optimize the softmax_weights AND the embeddings.\n",
    "    # This is because the embeddings are defined as a variable quantity and the\n",
    "    # optimizer's `minimize` method will by default modify all variable quantities \n",
    "    # that contribute to the tensor it is passed.\n",
    "    # See docs on `tf.train.Optimizer.minimize()` for more details.\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(loss)\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate = 3.0).minimize(loss)\n",
    "\n",
    "    # Compute the similarity between minibatch examples and all embeddings.\n",
    "    # We use the cosine distance:\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(\n",
    "        normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 5.834519\n",
      "Nearest to 不管: 不乏, 偏偏, 唯, 苦心经营, 联想, 一箭, 渴, 甚感,\n",
      "Nearest to 戴佛斯: 洛克, 不看, 铸造, 容颜, 莱蒙, 龙穴, 设在, 能派,\n",
      "Nearest to 亲自: 懒人, 让开, 奉承, 不, 追溯, 蓝宝石, 热, 荆棘女王,\n",
      "Nearest to 看来: 不知疲倦, 眼眶里, 以示, 衰弱, 帷幕, 不佳, 木塞, 月门堡,\n",
      "Nearest to 学士: 听人, 沿海, 甚感, 庞然, 仍然, 首度, 屁, 全完,\n",
      "Nearest to 等: 他常, 招揽, 黑洞, 冰霜, 岸, 平安地, 不知, 背,\n",
      "Nearest to 铁卫: ⑹, 起不来, 往外, 爱上你, 次女, 有知, 角, 两次,\n",
      "Nearest to 维斯特洛: 贝壳, 火花, 银线, 乌云, 女人家, 份, 凿, 布偶,\n",
      "Nearest to 史塔克: 喔, 屁滚尿流, 开溜, 闪现, 落坡, 退出, 偷来, 纹章,\n",
      "Nearest to 提利昂: 好多年, 开朗, 铸造, 薄云, 渔民, 技士, 灰藓, 之际,\n",
      "Nearest to 琼恩: 完好, 注意力, 蜜酒, 小麦, 吧, 罗纳德, 当学徒, 取笑,\n",
      "Nearest to 长城: 额间, 同名, 啤酒, 冷风, 入河, 方面, 群星, 铁民是,\n",
      "Nearest to 南方: 探视, 美丽动人, 逃兵, 并肩而行, 荒山, 小床, 踹, 果肉,\n",
      "Nearest to 死亡: 人喊, 靠岸, 拐棍, 白玫瑰, 芭蕾, 出彩, 暴雪, 污染,\n",
      "Nearest to 家族: 温家, 打趣, 房产, 千万别, 四座, 走投无路, 水边, 悔不当初,\n",
      "Nearest to 支持: 哈格, 自始至终, 目瞪口呆, 无语, 水手, 绰号, 有过之而无不及, 纵声,\n",
      "Nearest to 愤怒: 女泉, 属, 趁此机会, 怒不可遏, 以图, 大忙, 关上门, 真不错,\n",
      "Average loss at step 2000: 4.464754\n",
      "Average loss at step 4000: 4.118656\n",
      "Average loss at step 6000: 3.976826\n",
      "Average loss at step 8000: 3.854873\n",
      "Average loss at step 10000: 3.800134\n",
      "Nearest to 不管: 酸楚, 偏偏, 提斯语, 严冬, 呢, 募集, 可是, 刚来,\n",
      "Nearest to 戴佛斯: 詹姆, 何必, 提利昂, 凯特琳, 残渣, 山姆, 陛, 王太子,\n",
      "Nearest to 亲自: 让开, 蓝宝石, 头去, 更教, 奉承, 咱俩, 听懂, 时尚,\n",
      "Nearest to 看来: 衰弱, 不知疲倦, 气象, 此等, 眼眶里, 羞, 愠怒, 不佳,\n",
      "Nearest to 学士: 同伴, 甚感, 好运, 暗巷, 镶金, 马匹, 奖赏, 磕,\n",
      "Nearest to 等: 大江, 决, 施施然, 搭帐篷, 有无, 如果, 大不相同, 蓝底,\n",
      "Nearest to 铁卫: ⑹, 起不来, 两名, 崎岖, 次女, 气息, 厄拉兹, 转将,\n",
      "Nearest to 维斯特洛: 渊凯, =, 箭术, 乌云, 视察, 查出来, 能行, 银线,\n",
      "Nearest to 史塔克: 兰尼斯特, 熊熊烈火, 底下, 贝里斯, 波顿, 影影绰绰, 巍峨, 谈起,\n",
      "Nearest to 提利昂: 詹姆, 丹妮, 布蕾妮, 地, 群鸦, 瑟曦, 戴佛斯, 这是,\n",
      "Nearest to 琼恩: 援兵, 席恩, 山姆, 不肯, 丹妮, 雪诺, 她, 顶部,\n",
      "Nearest to 长城: 哪儿, 挥挥手, 活龙, 很浓, 空无一人, 法丽丝, 君临, 沙子,\n",
      "Nearest to 南方: 美丽动人, 探视, 小床, 逃兵, 双层, 贫穷, 快马, 果肉,\n",
      "Nearest to 死亡: 人喊, 芭蕾, 霍伍, 人上, 断肢, 拐棍, 含义, 靠岸,\n",
      "Nearest to 家族: 家, 夫人, 伯爵, 新闻, 鬼祟, 爵士, 丰饶, 畏惧,\n",
      "Nearest to 支持: 目瞪口呆, 哈格, 自始至终, 无语, 波及, 青少年, 水手, 绰号,\n",
      "Nearest to 愤怒: 怒不可遏, 驮马, 女泉, 饱经, 轻蔑, 大忙, 放肆, 从瓦兰,\n",
      "Average loss at step 12000: 3.719572\n",
      "Average loss at step 14000: 3.666779\n",
      "Average loss at step 16000: 3.635601\n",
      "Average loss at step 18000: 3.590950\n",
      "Average loss at step 20000: 3.554755\n",
      "Nearest to 不管: 刚来, 容许, 怎么回事, 酸楚, 身首异处, 觉醒, 触目惊心, 究竟,\n",
      "Nearest to 戴佛斯: 凯特琳, 提利昂, 詹姆, 丹妮, 艾莉亚, 山姆, 游牧民族, 将会,\n",
      "Nearest to 亲自: 其, 蓝宝石, 晴空, 让开, 否则, 更教, 亲手, 时尚,\n",
      "Nearest to 看来: 气象, 恐怕, 因此, 理当, 储存, 拜伦, 不堪重负, 德里克,\n",
      "Nearest to 学士: 甚感, 同伴, 门户, 兰德, 救火, 涌动, 十一岁, 孤儿,\n",
      "Nearest to 等: 施施然, 此刻, 大不相同, 毒手, 谢谢, 多久, 勇, 古瓦兰,\n",
      "Nearest to 铁卫: ⑹, 起不来, 次女, 两名, 葛洛, 泥灰, 动心, 崎岖,\n",
      "Nearest to 维斯特洛: 最小, 渊凯, 北方, 看守, 此, 拉克, 君临, 噩梦般,\n",
      "Nearest to 史塔克: 总兵力, 兰尼斯特, 影影绰绰, 太丑, 波顿, 糙, 过多, 艾林,\n",
      "Nearest to 提利昂: 詹姆, 丹妮, 瑟曦, 戴佛斯, 珊莎, 琼恩, 太后, 布蕾妮,\n",
      "Nearest to 琼恩: 丹妮, 珊莎, 提利昂, 布蕾妮, 瑟曦, 詹姆, 山姆, 想不出,\n",
      "Nearest to 长城: 我们, 城堡, 南方, 壁炉, 城墙, 南边, 天经地义, 君临,\n",
      "Nearest to 南方: 长城, 多恩, 易守难, 美丽动人, 自以为是, 果肉, 依稀, 哑着,\n",
      "Nearest to 死亡: 人上, 呆望, 人喊, 芭蕾, 由史, 梳到, 纸上, 请示,\n",
      "Nearest to 家族: 家, 夫人, 畏惧, 家人, 呼噜, 爵士, 军, 粉嘟嘟,\n",
      "Nearest to 支持: 目瞪口呆, 波及, 一枪, 家, 哈格, 青少年, 细发, 促狭,\n",
      "Nearest to 愤怒: 怒不可遏, 轻蔑, 驮马, 女泉, 从瓦兰, 各带, 迷宫, 饱经,\n",
      "Average loss at step 22000: 3.530367\n",
      "Average loss at step 24000: 3.503613\n",
      "Average loss at step 26000: 3.464918\n",
      "Average loss at step 28000: 3.453001\n",
      "Average loss at step 30000: 3.418546\n",
      "Nearest to 不管: 募集, 不当, 严冬, 遗嘱, 触目惊心, 身首异处, 酸楚, 失明,\n",
      "Nearest to 戴佛斯: 渥斯, 詹姆, 丹妮, 舍妹, 恶化, 凯特琳, 游牧民族, 洗礼,\n",
      "Nearest to 亲自: 否则, 洁娜, 让开, 立刻, 继续, 第三次, 一大早, 有利,\n",
      "Nearest to 看来: 德里克, 这么, 荒, 不该, 临空, 重围, 撒进, 此等,\n",
      "Nearest to 学士: 门户, 师傅, 同伴, 从赛, 海滩, 曾几何时, 狮身, 直穿,\n",
      "Nearest to 等: 谢谢, 戴, 六形, 穿不下, 夹, 回答, 毒手, 摺,\n",
      "Nearest to 铁卫: 动心, 针线, 次女, ⑹, 起不来, 两名, 葛洛, 心痛,\n",
      "Nearest to 维斯特洛: 北方, 最小, 渊凯, 此, 盘查, 提瑞, 居住, 神木,\n",
      "Nearest to 史塔克: 艾林, 波顿, 兰尼斯特, 第四任, 陶哈, 欠条, 总兵力, 长嗥,\n",
      "Nearest to 提利昂: 詹姆, 布蕾妮, 瑟曦, 艾莉亚, 珊莎, 阿莲, 发病, 山姆,\n",
      "Nearest to 琼恩: 詹姆, 牧师, 沃顿, 奈德, 石蛇, 泰陀斯, 磐, 珊莎,\n",
      "Nearest to 长城: 城墙, 活龙, 这里, 冰墙, 轮换, 口舌, 奔流, 很浓,\n",
      "Nearest to 南方: 果肉, 神木, 自以为是, 易守难, 例子, 君临, 多恩, 毫无二致,\n",
      "Nearest to 死亡: 助益, 人上, 曼斯, 竭尽, 呆望, 老奇斯, 人喊, 芭蕾,\n",
      "Nearest to 家族: 家, 虚构, 夫人, 温家, 畏惧, 提斯, 拥有, 叮叮,\n",
      "Nearest to 支持: 目瞪口呆, 估算, 血统, 笑容, 冰牢里, 夺来, 哈格, 事关,\n",
      "Nearest to 愤怒: 轻蔑, 怒不可遏, 驮马, 恐惧, 古怪, 轻微, 七弦, 梁柱,\n",
      "Average loss at step 32000: 3.407491\n",
      "Average loss at step 34000: 3.407006\n",
      "Average loss at step 36000: 3.362899\n",
      "Average loss at step 38000: 3.351379\n",
      "Average loss at step 40000: 3.338311\n",
      "Nearest to 不管: 最具, 遗嘱, 雪诺, 哪天, 传令, 欧兹, 募集, 总之,\n",
      "Nearest to 戴佛斯: 洗礼, 詹姆, 游牧民族, 威曼, 艾莉亚, 山前, 凯特琳, 瑟曦,\n",
      "Nearest to 亲自: 亲手, 发誓, 立刻, 赶上, 先, 而定, 弯弯的, 洁娜,\n",
      "Nearest to 看来: 也许, 真是, 玛蕊莲, 呵, 鸡奸, 假如, 地冲进, 哈哈,\n",
      "Nearest to 学士: 门户, 法兰, 师傅, 曾几何时, 从赛, 不当, 同伴, 直穿,\n",
      "Nearest to 等: 请问, \n",
      ", 签完, 施施然, 上岸时, 以后, 盯, 看看,\n",
      "Nearest to 铁卫: 同居, 心痛, 彻头彻尾, 次女, 泥灰, 动心, 多余, 艾拉,\n",
      "Nearest to 维斯特洛: 雇佣, 居住, 北方, 渊凯, 神木, 盘查, 圣恩, 捍卫,\n",
      "Nearest to 史塔克: 兰尼斯特, 克皮, 徒利, 欠条, 艾林, 单独, 波顿, 小妹,\n",
      "Nearest to 提利昂: 詹姆, 丹妮, 阿莲, 席恩, 他, 怒冲冲, 艾莉亚, 朴实无华,\n",
      "Nearest to 琼恩: 耶哥蕊特, 艾莉亚, 说得好, 有酒, 装神弄鬼, 丹妮, 韦伍, 十塔,\n",
      "Nearest to 长城: 城门, 广场, 城堡, 南边, 长凳, 战甲, 阳台, 小溪,\n",
      "Nearest to 南方: 神木, 多恩, 北方, 滋滋, 自以为是, 旧镇, 北境, 果肉,\n",
      "Nearest to 死亡: 助益, 人上, 来临, 供应, 胜利, 掀翻, 毙, 水城,\n",
      "Nearest to 家族: 家, 家人, 伯爵, 夫人, 虚构, 粉嘟嘟, 滑下来, 战俘,\n",
      "Nearest to 支持: 目瞪口呆, 命令, 忘记, 答谢, 坐等, 促狭, 拜拉席恩, 揭竿而起,\n",
      "Nearest to 愤怒: 痛苦, 怒不可遏, 轻蔑, 各带, 阴郁, 恐惧, 饱经, 甜美,\n",
      "Average loss at step 42000: 3.302132\n",
      "Average loss at step 44000: 3.313176\n",
      "Average loss at step 46000: 3.279113\n",
      "Average loss at step 48000: 3.275650\n",
      "Average loss at step 50000: 3.265336\n",
      "Nearest to 不管: 募集, 墙外, 总之, 雪诺, 不当, 最具, 无论, 事后,\n",
      "Nearest to 戴佛斯: 丹妮, 琼恩, 凯特琳, 渥斯, 威曼, 跑不了, 游牧民族, 伯爵夫人,\n",
      "Nearest to 亲自: 亲手, 钝剑, 困, 率领, 赶上, 弯弯的, 洁娜, 一门,\n",
      "Nearest to 看来: 天敌, 一文不值, 心知肚明, 不堪重负, 事故, 恐怕, 倘若, 曲线,\n",
      "Nearest to 学士: 法兰, 同伴, 师傅, 门户, 文家, 甚感, 天籁, 年老,\n",
      "Nearest to 等: 绑, 盯, 造就, 来来回回, 馨香, 施施然, 签完, 挂,\n",
      "Nearest to 铁卫: 同居, 波吉, 环扣, 动心, 东境, 小巷, 金牙, 艾拉,\n",
      "Nearest to 维斯特洛: 北方, 盘查, 建立, 圣恩, 拉克, 雇佣, 王国, 寝室,\n",
      "Nearest to 史塔克: 波顿, 世代, 艾林, 拉德, 顺遂, 欠条, 砍杀, 五只,\n",
      "Nearest to 提利昂: 丹妮, 詹姆, 小指头, 她, 虚张声势, 凯冯, 他, 蜂拥而上,\n",
      "Nearest to 琼恩: 戴佛斯, 丹妮, 耶哥蕊特, 牧师, 奈德, 山姆, 珊莎, 席恩,\n",
      "Nearest to 长城: 口舌, 神庙, 东方, 稻草堆, 阳台, 佛罗理, 座位, 屠刀,\n",
      "Nearest to 南方: 奔流, 神木, 地待, 滋滋, 临冬城, 多恩, 北方, 暮,\n",
      "Nearest to 死亡: 助益, 人上, 供应, 水城, 不可否认, 掀翻, 来临, 驯马,\n",
      "Nearest to 家族: 家, 坦通, 夫人, 寇, 风头, 瑞卡, 猎鹰, 十人,\n",
      "Nearest to 支持: 目瞪口呆, 拜拉席恩, 效忠, 踏进, 答谢, 塑像, 波及, 急迫,\n",
      "Nearest to 愤怒: 痛苦, 怒不可遏, 震惊, 恐惧, 各带, 不备, 阴郁, 吼声,\n",
      "Average loss at step 52000: 3.245063\n",
      "Average loss at step 54000: 3.250434\n",
      "Average loss at step 56000: 3.219503\n",
      "Average loss at step 58000: 3.203467\n",
      "Average loss at step 60000: 3.212557\n",
      "Nearest to 不管: 无论, 偏偏, 挤挤, 关系, 募集, 不待, 到底, 评论,\n",
      "Nearest to 戴佛斯: 丹妮, 凯特琳, 山姆, 提利昂, 阿莎, 游牧民族, 耕地, 疾行,\n",
      "Nearest to 亲自: 亲手, 熟练地, 而定, 赶上, 立刻, 长期, 饿鬼, 遭到,\n",
      "Nearest to 看来: 未停, 看, 特质, 这么回事, 一文不值, 缝, 无论, 真是,\n",
      "Nearest to 学士: 师傅, 法兰, 直穿, 同伴, 门户, 老师傅, 世, 洛斯,\n",
      "Nearest to 等: 可言, 求情, 盯, 祝, 行经, 有损于, 大江, 打油诗,\n",
      "Nearest to 铁卫: 东境, 环扣, 动心, 旁若无人, 明早, 饿鬼, 先上, 依靠,\n",
      "Nearest to 维斯特洛: 北方, 真正, 两者之间, 权力, 七大, 雇佣, 最小, 小泰,\n",
      "Nearest to 史塔克: 更远, 艾林, 欠条, 克皮, 世代, 兰尼斯特, 第四任, 拉德,\n",
      "Nearest to 提利昂: 布蕾妮, 丹妮, 詹姆, 珊莎, 凯特琳, 太后, 戴佛斯, 铁卫去,\n",
      "Nearest to 琼恩: 山姆, 珊莎, 艾莉亚, 耶哥蕊特, 格里芬, 提利昂, 他, 长爪,\n",
      "Nearest to 长城: 城墙, 龟盾, 献祭, 广场, 座位, 奔流, 林子里, 典礼,\n",
      "Nearest to 南方: 北方, 滋滋, 神木, 旧镇, 血腥气, 瓦兰, 微风, 东方,\n",
      "Nearest to 死亡: 助益, 名讳, 供应, 研磨, 强风, 辩解, 疯王, 毙,\n",
      "Nearest to 家族: 家, 夫人, 爵士, 叮叮, 粉嘟嘟, 家人, 虚构, 雷加,\n",
      "Nearest to 支持: 拜拉席恩, 效忠, 麾下, 公爵, 提离, 时刻, 塑像, 波及,\n",
      "Nearest to 愤怒: 恐惧, 痛苦, 吼声, 怒不可遏, 沉默, 阴郁, 轻微, 轻蔑,\n",
      "Average loss at step 62000: 3.201284\n",
      "Average loss at step 64000: 3.171199\n",
      "Average loss at step 66000: 3.189135\n",
      "Average loss at step 68000: 3.151941\n",
      "Average loss at step 70000: 3.172128\n",
      "Nearest to 不管: 偏偏, 挤挤, 募集, 我要, 以图, 依旧, 不待, 关系,\n",
      "Nearest to 戴佛斯: 他, 丹妮, 凯特琳, 循声, 山姆, 游牧民族, 科本, 死亡,\n",
      "Nearest to 亲自: 熟练地, 迎娶, 莱克, 而定, 饿鬼, 钝剑, 亲手, 连忙,\n",
      "Nearest to 看来: 蔬果, 重围, 真是, 鸡奸, 浅浅, 呕出, 曲线, 正对着,\n",
      "Nearest to 学士: 师傅, 法兰, 林务官, 直穿, 谷仓, 重拳, 霜冻, 门户,\n",
      "Nearest to 等: 恨透了, 唱唱, 指引, 伸向, 上星期, 可言, 盯, 载,\n",
      "Nearest to 铁卫: 动心, 荣誉, 同居, 先上, 小巷, 环扣, 别无他物, 东境,\n",
      "Nearest to 维斯特洛: 追得, 北方, 雇佣, 圣恩, 两者之间, 军械库, 权力, 瓦雷利亚,\n",
      "Nearest to 史塔克: 兰尼斯特, 第四任, 小妹, 塔斯, 亚丽, 波顿, 招子, 飘零,\n",
      "Nearest to 提利昂: 詹姆, 瑟曦, 她, 他, 珊莎, 凯特琳, 丹妮, 阿莲,\n",
      "Nearest to 琼恩: 长爪, 山姆, 艾莉亚, 文笔, 耶哥蕊特, 列神岛, 他, 第四次,\n",
      "Nearest to 长城: 城墙, 护身符, 广场, 城垛, 护城河, 维水, 龟盾, 活龙,\n",
      "Nearest to 南方: 北方, 滋滋, 多恩, 血腥气, 神木, 滑溜溜, 微风, 西境,\n",
      "Nearest to 死亡: 助益, 名讳, 戴佛斯, 研磨, 人上, 水城, 滴血, 白港,\n",
      "Nearest to 家族: 家, 爵士, 伯爵夫人, 坦通, 伯爵, 夫人, 族人, 呼噜,\n",
      "Nearest to 支持: 效忠, 势力, 顶撞, 打赢, 拜拉席恩, 麾下, 提离, 赦免,\n",
      "Nearest to 愤怒: 恐惧, 痛苦, 干净利落, 血偿, 吼声, 轻微, 绝望, 沉默,\n",
      "Average loss at step 72000: 3.137484\n",
      "Average loss at step 74000: 3.133467\n",
      "Average loss at step 76000: 3.136153\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-1081dc3f910e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m             data, batch_size, num_skips, slide_window)\n\u001b[1;32m     10\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtrain_dataset\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0maverage_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dong/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dong/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dong/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/dong/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dong/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 100001\n",
    "\n",
    "with tf.Session(graph=graph_sg) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_data, batch_labels = generate_batch_sg(\n",
    "            data, batch_size, num_skips, slide_window)\n",
    "        feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += l\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss = average_loss / 2000\n",
    "            print('Average loss at step %d: %f' % (step, average_loss))\n",
    "            average_loss = 0\n",
    "        \n",
    "        # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if step % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8 # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                log = 'Nearest to %s:' % valid_word\n",
    "                for k in range(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log = '%s %s,' % (log, close_word)\n",
    "                print(log)\n",
    "    final_embeddings_sg = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III-2. cbow 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "\n",
    "def generate_batch_cbow(data, batch_size, slide_window, num_skips):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        data: list_of_integer格式的文本\n",
    "        batch_size: 一个minibatch的样本大小\n",
    "        slide_window: 滑动窗口的大小，定义语境（context）的范围\n",
    "        num_skips: 语境(context)包含的单词数量\n",
    "        \n",
    "    Returns:\n",
    "        batch: 一个minibatch的语境\n",
    "        labels: 一个minibatch的标记\n",
    "    \"\"\"\n",
    "    \n",
    "    global data_index\n",
    "    \n",
    "    # 和skip-gram不同，\n",
    "    # cbow里面，输入的语境context 和 输出的label包含不同数量的单词\n",
    "    # 因为，一个样本包含 (1) num_skips个context单词 和 (2) 一个label单词\n",
    "    # 我们表示为 num_context 和 batch_size：\n",
    "    num_context = num_skips * batch_size\n",
    "    batch = np.ndarray(shape=(num_context), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    \n",
    "    # # 使用一个buffer记录和更新滑动窗，[ slide_window target slide_window ]\n",
    "    span = 2 * slide_window + 1 \n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    \n",
    "    # # 从每一个新的滑动窗提取一个训练样本:\n",
    "    # （num_skips个单词组成的语境context, 一个目标单词作为标记）\n",
    "    for i in range(batch_size):\n",
    "        rand_x = np.random.permutation(span)\n",
    "        if 2 * slide_window == num_skips:\n",
    "            for j in range(slide_window):\n",
    "                batch[i*num_skips + j] = buffer[j]\n",
    "            for j in range(slide_window, 2 * slide_window):\n",
    "                batch[i*num_skips + j] = buffer[j+1]\n",
    "        else:\n",
    "            j, k = 0, 0\n",
    "            for j in range(num_skips):\n",
    "                while rand_x[k]==slide_window:\n",
    "                    k += 1\n",
    "                batch[i * num_skips + j] = buffer[rand_x[k]]\n",
    "                k += 1\n",
    "        labels[i, 0] = buffer[slide_window]\n",
    "        \n",
    "        # 将滑动窗向右滑动随机步\n",
    "        rand_step = np.random.randint(1,5)\n",
    "        for _ in range(rand_step):\n",
    "            buffer.append(data[data_index])\n",
    "            data_index = (data_index + 1) % len(data)\n",
    "    \n",
    "    return batch, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "with num_skips = 2 and slide_window = 1:\n",
      "batch context size = (16,)\n",
      "不像 魔幻 ==> UNK\n",
      "魔幻 一样 ==> 小说\n",
      "小说 用 ==> 一样\n",
      "的 或是 ==> 魔法\n",
      "物种 吸引 ==> 来\n",
      "人们 眼球 ==> 的\n",
      "而是 跌宕起伏 ==> 讲究\n",
      "跌宕起伏 情节 ==> 的\n"
     ]
    }
   ],
   "source": [
    "# 测试：\n",
    "slide_window = 1\n",
    "num_skips = 2\n",
    "batch_size = 8\n",
    "\n",
    "batch, labels = generate_batch_cbow(data = data,\n",
    "                                    batch_size = batch_size,\n",
    "                                    slide_window = slide_window,\n",
    "                                    num_skips = num_skips)\n",
    "\n",
    "print('\\nwith num_skips = %d and slide_window = %d:' % (num_skips, slide_window))\n",
    "\n",
    "print('batch context size = %s' % repr(np.shape(batch)))\n",
    "for x in range(0, batch_size * num_skips, num_skips):\n",
    "    print(' '.join([reverse_dictionary[bi] for bi in batch[x:x+num_skips]])\n",
    "          + ' ==> '\n",
    "          + reverse_dictionary[labels[x//num_skips][0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 128 # 词向量维度\n",
    "num_sampled = 64   # Number of negative examples to sample.\n",
    "\n",
    "## cbow的hyperparameters：\n",
    "# 语境范围，考虑和周围多少个词语的共存关系\n",
    "slide_window = 1\n",
    "num_skips = 2\n",
    "\n",
    "# 产生测试数据\n",
    "valid_size = 8\n",
    "valid_examples = list(np.random.permutation(1000)[:valid_size])\n",
    "names = ['史塔克', '提利昂', '琼恩', '长城', '南方', '死亡', '家族', '支持', '愤怒']\n",
    "for name in names:\n",
    "    valid_examples.append(dictionary[name])\n",
    "    valid_size += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "graph_cbow = tf.Graph()\n",
    "with graph_cbow.as_default():\n",
    "    \n",
    "    ## 第一层：数据\n",
    "    # 训练数据分配内存空间\n",
    "    train_dataset = tf.placeholder(tf.int32, shape=[num_skips * batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    #　固定的测试数据，以tf.constant这种variable形式\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "    ## 第一层与第二层之间：词向量矩阵\n",
    "    # 一个tf.Variable对象，将整数ID映射为词向量特征\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    \n",
    "    ## 第二层：通过查询操作将context（整数ID的list）转化为词向量\n",
    "    # 通过对context里面的所有词的词向量们简单加和得到context的词向量表示\n",
    "    # shape = [ (batch_size * num_skip) x embedding_size ]\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "    # shape = [ batch_size x num_skip x embedding_size ]\n",
    "    embed = tf.reshape(embed, [batch_size, num_skips, -1])\n",
    "    # shape = [ batch_size x embedding_size]\n",
    "    final_embed = tf.reduce_sum(input_tensor = embed,\n",
    "                          axis = 1,\n",
    "                          keep_dims = False)\n",
    "    \n",
    "    ## 第三层：预测\n",
    "    # 给定context的词向量表示，预测target，和真实的target比较，计算loss\n",
    "    softmax_weights = tf.Variable(tf.truncated_normal(\n",
    "        [vocabulary_size, embedding_size],\n",
    "        stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.sampled_softmax_loss(weights = softmax_weights,\n",
    "                                   biases = softmax_biases,\n",
    "                                   inputs = final_embed,\n",
    "                                   labels = train_labels,\n",
    "                                   num_sampled = num_sampled,\n",
    "                                   num_classes = vocabulary_size))\n",
    "    \n",
    "    # 优化参数\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate = 0.01).minimize(loss)\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate = 1.0).minimize(loss)\n",
    "    \n",
    "    ## 测试模型：\n",
    "    #　\n",
    "    # Compute the similarity between minibatch validation examples and\n",
    "    #   all embeddings, using the cosine distance:\n",
    "    # \n",
    "    # 第一步： 对每个单词的词向量归一化\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    # 小心： must use argument 'keep_dims=True' to normalize correctly\n",
    "    normalized_embeddings = embeddings // norm\n",
    "    # 第二步： 对 valid_dataset 里面的每一个单词查询归一化的词向量\n",
    "    valid_embeddings = tf.nn.embedding_lookup(\n",
    "        normalized_embeddings, valid_dataset)\n",
    "    # 第三步： 通过内积计算 valid_dataset 里面的每一个单词和所有单词的cosine similarity\n",
    "    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0 is 8.7382516861\n",
      "Nearest to 所: 矛兵, 绷起, 摇身一变, 身材矮小, 搁下, 呵欠, 越趋, 宠幸,\n",
      "Nearest to 头: 粗大, 高墙, 一眼, 之牙上, 不见, 先要, 身材矮小, 波纹,\n",
      "Nearest to 认为: 红铜, 箭矢, 好时机, 偷笑, 箭靶, 北风, 矮马, 撮,\n",
      "Nearest to 转向: 拍打着, 八九天, 毫不犹豫, 我命, 故而, 睡不醒, 征税, 蹄铁,\n",
      "Nearest to 却: 亲随, 大营, 宝宝, 主子, 交给, 之神, 欣慰, 山头,\n",
      "Nearest to 战争: 症状, 睡着, 夭亡, 矛兵, 矮种, 糟糕透顶, 不见, 菌类,\n",
      "Nearest to 一直: 妓魇安柏, 波动, 磨得, 林子, 摇醒, 生涯, 费力, 用龙,\n",
      "Nearest to 一道: 银舌西蒙, 宝宝, 不生, 杀出, 办法, 八九天, 大营, 往下走,\n",
      "Nearest to 史塔克: 干酪, 往下走, 睡不醒, 止血, 摇醒, 辨出, 骇人, 恰当,\n",
      "Nearest to 提利昂: 许诺, 海龟, 心中, 摇醒, 几句话, 厚度, 止血, 饕餮,\n",
      "Nearest to 琼恩: 禽兽, 干酪, 症状, 罪孽, 糟糕透顶, 喝令, 宝宝, 飞离,\n",
      "Nearest to 长城: 摇醒, 拆除, 硬拖, 老得, 布鲁斯, 好容易, 木盾, 怒道,\n",
      "Nearest to 南方: 湖中, 胆大包天, 老得, 我哥, 龙太子, 河间, 伤腿, 木柄,\n",
      "Nearest to 死亡: 面露, 往下走, 不会错, 只算, 石头路, 富丽堂皇, 由, 洗涤,\n",
      "Nearest to 家族: 一簇, 帮帮, 发落, 飘落, 发着, 疹子, 倦怠, 矛兵,\n",
      "Nearest to 支持: 防御工事, 莫名其妙, 别管, 老外, 扳机, 黑焰, 摇醒, 放鹰,\n",
      "Nearest to 愤怒: 轻侮, 摇醒, 真多, 衣领, 地探, 乱打, 派娅妮, 深灰色,\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100001\n",
    "\n",
    "with tf.Session(graph=graph_cbow) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    print(\"Initialized\")\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        # generate minibatch of samples, feed into feed_dict\n",
    "        batch_data, batch_labels = generate_batch_cbow(data = data,\n",
    "                                    batch_size = batch_size,\n",
    "                                    slide_window = slide_window,\n",
    "                                    num_skips = num_skips)\n",
    "        \n",
    "        feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "        \n",
    "        # run session to calculate loss and optimize parameters\n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        \n",
    "        # 1. simple evaluation:\n",
    "        #   calculate and display average loss every 2000 minibatch\n",
    "        average_loss += l\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss = average_loss / 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print(\"Average loss at step %d is %s\" % (step,average_loss))\n",
    "            average_loss = 0\n",
    "        \n",
    "        # 2. expensive evaluation:\n",
    "        #   calculate cosine similarities every 10,000 minibatch\n",
    "        if step % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8 # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                log = \"Nearest to %s:\" % valid_word\n",
    "                for k in range(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log = \"%s %s,\" % (log, close_word)\n",
    "                print(log)\n",
    "    final_embeddings_cbow = normalized_embeddings.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
